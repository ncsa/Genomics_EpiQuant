# Current Status

## SPAEML

* SPAEML works given input formatted in a clean table structure where rows are sample names and columns are genotypes/phenotypes and sample names are identical between the two input files.
* The ANOVA table in the output is a Standard ANOVA table, and does not yet have AIC, BIC, mBIC yet included.
* Benchmarking had been done on iForge (using Spark Standalone), but since this benchmarking a major optimization was introduced to the code (ISSUE #6) and tests need to be redone.
* Before computing anything, the program first checks to see whether the directory that will hold the output already exists. If it does, the program exits. We do not want users to erase output files that took a lot of work to generate because they forgot to rename the output directory flag.

## Converters

* Little done for file conversion. We have a specialized tool that can transpose tsv data and insert a placeholder value in the top-left corner of the file to get it into the exact format that SPAEML needs

# Future Work

## Testing on Amazon

* Steven will begin scalability testing on Amazon, but he needs larger data sets than what we currently have, and will have to simulate them.
* He is experimenting with a tool called SeqSIMLA, which can simulate data and return data in PLINK's .ped/.map formats. So naturally, we will create the file parsers to deal with these simulated data sets, killing two birds with one stone.

## File Parsers

Create parsers that read in genotype and phenotype data in the following formats:

* PLINK's .ped/.map formats
* PLINK's .bed file (binary format)
* Others?

**Important design decision**

These parsers can either produce intermediate files in a standard format that all tools (SPAEML, LASSO, etc) can digest, or simply be attached to the beginning of each tool to parse data on the fly. If parsing data is very fast, creating an intermediate file will be unnecessary, but if it takes a long time, it makes sense to produce an intermediate file.

## LASSO

Need to explore the existing tools for running LASSO in Spark.
* Apparently, LASSO is implemented in Spark's RDD-based library (mlib; which may be deprecated in Spark3)
* There is no LASSO implementation in Spark's Dataframe library (ml)

Maybe in the interest of time, we should go ahead and use the mlib library for LASSO, even though we decided to not do that for the Ordinary Least Squares Regression in the SPAEML implementation (Implementing the stats ourselves took a tremendous amount of time)

## SPAEML

* Need to change how the input data is digested based on decisions made in the "File Parsers section"

## Overall design
